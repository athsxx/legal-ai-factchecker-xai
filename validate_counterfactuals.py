#!/usr/bin/env python3
"""
Counterfactual Accuracy Validation Framework
============================================

This script validates the accuracy and quality of counterfactual explanations
generated by the XAI system through multiple validation methods.
"""

import json
import requests
import time
from typing import Dict, List, Any, Tuple
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

class CounterfactualValidator:
    """Comprehensive validation framework for counterfactual explanations"""
    
    def __init__(self, ml_service_url: str = "http://localhost:8000"):
        self.ml_service_url = ml_service_url
        self.validation_results = {}
        
        # Test cases with known ground truth
        self.legal_test_cases = [
            {
                "text": "The party shall pay damages within 30 days of breach.",
                "expected_impact_changes": {
                    "'shall' ‚Üí 'may'": "high",  # Changes obligation strength
                    "'shall' ‚Üí 'will'": "high",  # Changes obligation strength
                    "'breach' ‚Üí 'compliance'": "high",  # Complete reversal
                    "'30 days' ‚Üí 'immediately'": "medium",  # Changes timeline
                    "'damages' ‚Üí 'compensation'": "low"  # Similar meaning
                },
                "expected_types": ["word_substitution", "contextual_scenario", "targeted_refute"],
                "legal_context": "contract"
            },
            {
                "text": "The defendant is liable for all damages resulting from negligence.",
                "expected_impact_changes": {
                    "'liable' ‚Üí 'not liable'": "high",  # Complete reversal
                    "'all' ‚Üí 'some'": "medium",  # Scope change
                    "'all' ‚Üí 'no'": "high",  # Complete reversal
                    "'negligence' ‚Üí 'gross negligence'": "medium"  # Severity change
                },
                "expected_types": ["word_substitution", "negation", "targeted_refute"],
                "legal_context": "liability"
            },
            {
                "text": "This agreement is binding and irrevocable for 5 years.",
                "expected_impact_changes": {
                    "'binding' ‚Üí 'non-binding'": "high",
                    "'binding' ‚Üí 'advisory'": "high", 
                    "'irrevocable' ‚Üí 'revocable'": "high",
                    "'irrevocable' ‚Üí 'conditional'": "high",
                    "'5 years' ‚Üí '1 year'": "medium"
                },
                "expected_types": ["word_substitution", "contextual_scenario"],
                "legal_context": "contract"
            }
        ]
        
    def validate_all(self) -> Dict[str, Any]:
        """Run comprehensive validation suite"""
        print("üß™ Starting Counterfactual Validation Suite...")
        print("=" * 60)
        
        results = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "service_health": self._check_service_health(),
            "semantic_validity": self._validate_semantic_validity(),
            "impact_assessment_accuracy": self._validate_impact_assessment(),
            "coverage_completeness": self._validate_coverage_completeness(),
            "consistency_tests": self._validate_consistency(),
            "expert_alignment": self._validate_expert_alignment(),
            "overall_score": 0.0
        }
        
        # Calculate overall score
        scores = [
            results["semantic_validity"]["score"],
            results["impact_assessment_accuracy"]["score"],
            results["coverage_completeness"]["score"],
            results["consistency_tests"]["score"],
            results["expert_alignment"]["score"]
        ]
        results["overall_score"] = np.mean([s for s in scores if s is not None])
        
        self._generate_report(results)
        return results
    
    def _check_service_health(self) -> Dict[str, Any]:
        """Verify ML service is running and responsive"""
        print("üîç Checking service health...")
        
        try:
            response = requests.get(f"{self.ml_service_url}/health", timeout=5)
            if response.status_code == 200:
                return {"status": "healthy", "response_time": response.elapsed.total_seconds()}
            else:
                return {"status": "unhealthy", "error": f"Status code: {response.status_code}"}
        except Exception as e:
            return {"status": "unreachable", "error": str(e)}
    
    def _validate_semantic_validity(self) -> Dict[str, Any]:
        """Test if counterfactuals maintain semantic coherence"""
        print("üìù Validating semantic validity...")
        
        results = {"tests": [], "score": 0.0, "details": {}}
        total_tests = 0
        passed_tests = 0
        
        for test_case in self.legal_test_cases:
            try:
                # Generate counterfactuals
                response = requests.post(
                    f"{self.ml_service_url}/generate-counterfactuals",
                    json={"claim": test_case["text"]},
                    timeout=30
                )
                
                if response.status_code != 200:
                    results["tests"].append({
                        "test": f"Semantic validity for: {test_case['text'][:50]}...",
                        "status": "failed",
                        "error": f"HTTP {response.status_code}"
                    })
                    continue
                
                data = response.json()
                counterfactuals = data.get("counterfactual_examples", [])
                
                # Test semantic coherence
                for cf in counterfactuals[:3]:  # Test first 3
                    total_tests += 1
                    
                    # Check if modified text is grammatically coherent
                    is_coherent = self._check_semantic_coherence(
                        test_case["text"], 
                        cf.get("text", "")
                    )
                    
                    if is_coherent:
                        passed_tests += 1
                    
                    results["tests"].append({
                        "original": test_case["text"],
                        "counterfactual": cf.get("text", ""),
                        "coherent": is_coherent,
                        "changes": cf.get("changes", []),
                        "type": cf.get("type", "unknown")
                    })
                    
            except Exception as e:
                results["tests"].append({
                    "test": f"Semantic validity for: {test_case['text'][:50]}...",
                    "status": "error",
                    "error": str(e)
                })
        
        results["score"] = passed_tests / total_tests if total_tests > 0 else 0.0
        results["details"] = {
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "pass_rate": results["score"]
        }
        
        print(f"   ‚úÖ Semantic validity: {results['score']:.2%} ({passed_tests}/{total_tests})")
        return results
    
    def _validate_impact_assessment(self) -> Dict[str, Any]:
        """Validate accuracy of impact level assignments"""
        print("‚öñÔ∏è Validating impact assessment accuracy...")
        
        results = {"tests": [], "score": 0.0, "confusion_matrix": {}}
        correct_predictions = 0
        total_predictions = 0
        
        for test_case in self.legal_test_cases:
            try:
                response = requests.post(
                    f"{self.ml_service_url}/generate-counterfactuals",
                    json={"claim": test_case["text"]},
                    timeout=30
                )
                
                if response.status_code != 200:
                    continue
                
                data = response.json()
                counterfactuals = data.get("counterfactual_examples", [])
                
                # Check impact assessments against expected values
                for cf in counterfactuals:
                    changes = cf.get("changes", [])
                    predicted_impact = cf.get("impact", "unknown")
                    
                    # Find matching expected impact - check each change individually
                    for change in changes:
                        if change in test_case["expected_impact_changes"]:
                            expected_impact = test_case["expected_impact_changes"][change]
                            total_predictions += 1
                            
                            # Normalize impact levels for comparison
                            predicted_normalized = predicted_impact.lower()
                            expected_normalized = expected_impact.lower()
                            
                            # Handle "moderate" as "medium"
                            if predicted_normalized == "moderate":
                                predicted_normalized = "medium"
                            
                            is_correct = predicted_normalized == expected_normalized
                            
                            if is_correct:
                                correct_predictions += 1
                            
                            results["tests"].append({
                                "text": test_case["text"],
                                "change": change,
                                "predicted_impact": predicted_impact,
                                "expected_impact": expected_impact,
                                "correct": is_correct
                            })
                        
            except Exception as e:
                print(f"   ‚ùå Error testing impact assessment: {e}")
        
        results["score"] = correct_predictions / total_predictions if total_predictions > 0 else 0.0
        
        print(f"   ‚úÖ Impact assessment accuracy: {results['score']:.2%} ({correct_predictions}/{total_predictions})")
        return results
    
    def _validate_coverage_completeness(self) -> Dict[str, Any]:
        """Test if all expected counterfactual types are generated"""
        print("üìä Validating coverage completeness...")
        
        results = {"tests": [], "score": 0.0, "coverage_analysis": {}}
        total_expected_types = 0
        found_types = 0
        
        for test_case in self.legal_test_cases:
            try:
                response = requests.post(
                    f"{self.ml_service_url}/generate-counterfactuals",
                    json={"claim": test_case["text"]},
                    timeout=30
                )
                
                if response.status_code != 200:
                    continue
                
                data = response.json()
                counterfactuals = data.get("counterfactual_examples", [])
                generated_types = set(cf.get("type", "unknown") for cf in counterfactuals)
                expected_types = set(test_case["expected_types"])
                
                total_expected_types += len(expected_types)
                found_types += len(generated_types & expected_types)
                
                results["tests"].append({
                    "text": test_case["text"],
                    "expected_types": list(expected_types),
                    "generated_types": list(generated_types),
                    "coverage": len(generated_types & expected_types) / len(expected_types),
                    "missing_types": list(expected_types - generated_types)
                })
                
            except Exception as e:
                print(f"   ‚ùå Error testing coverage: {e}")
        
        results["score"] = found_types / total_expected_types if total_expected_types > 0 else 0.0
        
        print(f"   ‚úÖ Coverage completeness: {results['score']:.2%} ({found_types}/{total_expected_types})")
        return results
    
    def _validate_consistency(self) -> Dict[str, Any]:
        """Test consistency across multiple runs"""
        print("üîÑ Validating consistency across runs...")
        
        results = {"tests": [], "score": 0.0, "stability_metrics": {}}
        consistency_scores = []
        
        test_text = self.legal_test_cases[0]["text"]  # Use first test case
        
        # Run multiple times and check consistency
        all_runs = []
        for run in range(3):
            try:
                response = requests.post(
                    f"{self.ml_service_url}/generate-counterfactuals",
                    json={"claim": test_text},
                    timeout=30
                )
                
                if response.status_code == 200:
                    data = response.json()
                    counterfactuals = data.get("counterfactual_examples", [])
                    all_runs.append(counterfactuals)
                    
            except Exception as e:
                print(f"   ‚ùå Error in consistency test run {run + 1}: {e}")
        
        if len(all_runs) >= 2:
            # Compare consistency between runs
            consistency_score = self._calculate_consistency(all_runs)
            consistency_scores.append(consistency_score)
        
        results["score"] = np.mean(consistency_scores) if consistency_scores else 0.0
        
        print(f"   ‚úÖ Consistency score: {results['score']:.2%}")
        return results
    
    def _validate_expert_alignment(self) -> Dict[str, Any]:
        """Validate alignment with legal expert knowledge"""
        print("üë®‚Äç‚öñÔ∏è Validating expert alignment (heuristic-based)...")
        
        # This is a simplified heuristic-based validation
        # In practice, you would compare against expert annotations
        results = {"tests": [], "score": 0.0, "expert_rules": {}}
        
        expert_rules = [
            ("Legal obligation modifiers (shall/may/must) should have HIGH impact", 
             lambda cf: any("shall" in str(change).lower() or "may" in str(change).lower() or "must" in str(change).lower() 
                          for change in cf.get('changes', [])) and cf.get('impact', '').lower() == 'high'),
            ("Liability changes (liable/not liable) should have HIGH impact",
             lambda cf: any("liable" in str(change).lower() for change in cf.get('changes', [])) and cf.get('impact', '').lower() == 'high'),
            ("Binding/revocable changes should have HIGH impact",
             lambda cf: any(word in str(cf.get('changes', [])).lower() for word in ['binding', 'irrevocable', 'revocable']) and cf.get('impact', '').lower() == 'high'),
            ("Complete reversal changes should have HIGH impact",
             lambda cf: any('‚Üí not' in str(change) or 'not liable' in str(change).lower() or 'non-binding' in str(change).lower() 
                          for change in cf.get('changes', [])) and cf.get('impact', '').lower() == 'high'),
            ("Scope changes (all/some/no) should have MEDIUM-HIGH impact",
             lambda cf: any(word in str(cf.get('changes', [])).lower() for word in ['all', 'some', 'no', 'none']) and cf.get('impact', '').lower() in ['medium', 'high']),
            ("Temporal absoluteness changes should have MEDIUM-HIGH impact",
             lambda cf: any(word in str(cf.get('changes', [])).lower() for word in ['immediately', 'never', 'always', 'forthwith']) and cf.get('impact', '').lower() in ['medium', 'high']),
            ("Requirement strength changes (required/optional) should have HIGH impact",
             lambda cf: any(word in str(cf.get('changes', [])).lower() for word in ['required', 'optional', 'mandatory']) and cf.get('impact', '').lower() == 'high'),
            ("Validity changes (void/valid) should have HIGH impact",
             lambda cf: any(word in str(cf.get('changes', [])).lower() for word in ['void', 'valid', 'enforceable']) and cf.get('impact', '').lower() == 'high'),
            ("Low-impact synonyms should have LOW impact",
             lambda cf: any(synonym_pair in str(cf.get('changes', [])).lower() 
                          for synonym_pair in ['damages ‚Üí compensation', 'breach ‚Üí violation', 'responsible ‚Üí accountable']) 
                          and cf.get('impact', '').lower() == 'low'),
            ("Negation type counterfactuals should typically have HIGH impact",
             lambda cf: cf.get('type') == 'negation' and cf.get('impact', '').lower() in ['high', 'medium']),
        ]
        
        total_rules = len(expert_rules) * len(self.legal_test_cases)
        rules_satisfied = 0
        
        for test_case in self.legal_test_cases:
            try:
                response = requests.post(
                    f"{self.ml_service_url}/generate-counterfactuals",
                    json={"claim": test_case["text"]},
                    timeout=30
                )
                
                if response.status_code != 200:
                    continue
                
                data = response.json()
                counterfactuals = data.get("counterfactual_examples", [])
                
                for rule_name, rule_func in expert_rules:
                    rule_satisfied = False
                    for cf in counterfactuals:
                        if rule_func(cf):
                            rule_satisfied = True
                            break
                    
                    if rule_satisfied:
                        rules_satisfied += 1
                    
                    results["tests"].append({
                        "rule": rule_name,
                        "text": test_case["text"],
                        "satisfied": rule_satisfied
                    })
                    
            except Exception as e:
                print(f"   ‚ùå Error in expert alignment test: {e}")
        
        results["score"] = rules_satisfied / total_rules if total_rules > 0 else 0.0
        
        print(f"   ‚úÖ Expert alignment: {results['score']:.2%} ({rules_satisfied}/{total_rules})")
        return results
    
    def _check_semantic_coherence(self, original: str, modified: str) -> bool:
        """Simple heuristic to check if modified text is semantically coherent"""
        # Basic checks
        if not modified or len(modified.strip()) == 0:
            return False
        
        # Check if it's still a legal sentence structure
        legal_indicators = ['shall', 'may', 'must', 'liable', 'damages', 'breach', 'party', 'agreement']
        has_legal_context = any(word in modified.lower() for word in legal_indicators)
        
        # Check if sentence structure is maintained
        has_subject_verb = len(modified.split()) >= 3
        
        return has_legal_context and has_subject_verb
    
    def _calculate_consistency(self, runs: List[List[Dict]]) -> float:
        """Calculate consistency score between multiple runs"""
        if len(runs) < 2:
            return 0.0
        
        # Compare types generated across runs
        types_per_run = [set(cf.get('type', 'unknown') for cf in run) for run in runs]
        
        # Calculate Jaccard similarity between consecutive runs
        similarities = []
        for i in range(len(types_per_run) - 1):
            intersection = len(types_per_run[i] & types_per_run[i + 1])
            union = len(types_per_run[i] | types_per_run[i + 1])
            similarity = intersection / union if union > 0 else 0.0
            similarities.append(similarity)
        
        return np.mean(similarities)
    
    def _generate_report(self, results: Dict[str, Any]) -> None:
        """Generate comprehensive validation report"""
        print("\n" + "=" * 60)
        print("üìã COUNTERFACTUAL VALIDATION REPORT")
        print("=" * 60)
        
        print(f"‚è∞ Timestamp: {results['timestamp']}")
        print(f"üè• Service Health: {results['service_health']['status']}")
        print(f"üéØ Overall Accuracy Score: {results['overall_score']:.1%}")
        
        print("\nüìä DETAILED SCORES:")
        print("-" * 40)
        print(f"‚Ä¢ Semantic Validity:      {results['semantic_validity']['score']:.1%}")
        print(f"‚Ä¢ Impact Assessment:      {results['impact_assessment_accuracy']['score']:.1%}")
        print(f"‚Ä¢ Coverage Completeness:  {results['coverage_completeness']['score']:.1%}")
        print(f"‚Ä¢ Consistency:            {results['consistency_tests']['score']:.1%}")
        print(f"‚Ä¢ Expert Alignment:       {results['expert_alignment']['score']:.1%}")
        
        # Recommendations
        print("\nüí° RECOMMENDATIONS:")
        print("-" * 40)
        
        if results['overall_score'] >= 0.8:
            print("‚úÖ Excellent: Counterfactuals show high accuracy and reliability")
        elif results['overall_score'] >= 0.6:
            print("‚ö†Ô∏è  Good: Some improvements needed in specific areas")
        else:
            print("‚ùå Needs Improvement: Significant accuracy issues detected")
        
        # Specific recommendations
        if results['semantic_validity']['score'] < 0.7:
            print("   ‚Ä¢ Improve semantic coherence checking")
        if results['impact_assessment_accuracy']['score'] < 0.7:
            print("   ‚Ä¢ Refine impact assessment logic")
        if results['coverage_completeness']['score'] < 0.7:
            print("   ‚Ä¢ Expand counterfactual type generation")
        
        # Save detailed results
        report_file = Path("counterfactual_validation_report.json")
        with open(report_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        print(f"\nüíæ Detailed report saved to: {report_file}")
        print("=" * 60)

    def quick_test(self, text: str) -> Dict[str, Any]:
        """Quick test of a single text for manual validation"""
        print(f"üîç Quick Test: {text}")
        
        try:
            response = requests.post(
                f"{self.ml_service_url}/generate-counterfactuals",
                json={"claim": text},
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                counterfactuals = data.get("counterfactual_examples", [])
                
                print(f"Generated {len(counterfactuals)} counterfactuals:")
                for i, cf in enumerate(counterfactuals[:5], 1):
                    print(f"  {i}. {cf.get('text', 'N/A')}")
                    print(f"     Type: {cf.get('type', 'N/A')}, Impact: {cf.get('impact', 'N/A')}")
                    print(f"     Changes: {', '.join(cf.get('changes', []))}")
                    print()
                
                return data
            else:
                print(f"‚ùå Error: HTTP {response.status_code}")
                return {}
                
        except Exception as e:
            print(f"‚ùå Error: {e}")
            return {}

def main():
    """Run the validation suite"""
    print("üß™ Counterfactual Accuracy Validation")
    print("=====================================")
    
    # Initialize validator
    validator = CounterfactualValidator()
    
    # Quick service check
    print("\n1Ô∏è‚É£ Quick Service Check...")
    health = validator._check_service_health()
    if health["status"] != "healthy":
        print(f"‚ùå Service not available: {health}")
        return
    
    # Run quick test
    print("\n2Ô∏è‚É£ Quick Test Example...")
    validator.quick_test("The party shall pay damages within 30 days of breach.")
    
    # Ask user if they want full validation
    print("\n3Ô∏è‚É£ Full Validation Suite")
    user_input = input("Run full validation suite? (y/n): ").strip().lower()
    
    if user_input == 'y':
        results = validator.validate_all()
        
        # Additional analysis
        if results["overall_score"] >= 0.7:
            print("\nüéâ CONCLUSION: Counterfactual explanations show good accuracy!")
            print("The system can be trusted for legal document analysis.")
        else:
            print("\n‚ö†Ô∏è CONCLUSION: Improvements needed in counterfactual accuracy.")
            print("Consider refining the generation algorithms.")

if __name__ == "__main__":
    main()
